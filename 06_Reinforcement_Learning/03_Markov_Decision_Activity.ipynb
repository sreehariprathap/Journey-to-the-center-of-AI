{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARKOV DECISION PROCESS : 2x2 MAZE\n",
    "\n",
    "### Introduction\n",
    "Markov Decision Processes (MDPs)\n",
    "\n",
    "A Markov Decision Process (MDP) is a framework used in reinforcement learning to model decision-making problems. The agent interacts with an environment, taking actions to transition between states while receiving rewards to maximize its long-term return. An MDP is defined by:\n",
    "\n",
    "- **States ($S$)**: The set of all possible states of the environment.\n",
    "- **Actions ($A$)**: The set of all possible actions the agent can take.\n",
    "- **Transition Probabilities ($P(s' \\mid s, a)$)**: The probability of transitioning to state $s'$ from state $s$ by taking action $a$.\n",
    "- **Rewards ($R(s, a, s')$)**: The immediate reward received after transitioning to $s'$.\n",
    "- **Policy ($\\pi(a \\mid s)$)**: A probability distribution over actions given the current state.\n",
    "\n",
    "The goal is to find the optimal policy $\\pi^*$ that maximizes the expected return, which is the cumulative discounted reward over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards, Returns, Long-Term Values, and Policies in 2x2 Maze\n",
    "Maze Layout\n",
    "We use the following 2x2 grid as our environment:\n",
    "\n",
    "\n",
    "\n",
    "| s1 | s2 |\n",
    "|----|----|\n",
    "| s3 | s4 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start state: $s_1$\n",
    "\n",
    "Goal state: $s_4$ (reward = $+1$)\n",
    "\n",
    "Other states: No rewards ($R=0$)\n",
    "\n",
    "Actions: $A=\\{ \\text{up, down, left, right} \\}$\n",
    "\n",
    "Terminal state: $s_4$, no further actions.\n",
    "\n",
    "MDP Transition Probabilities $P(s' \\mid s, a)$\n",
    "\n",
    "Here are examples of deterministic transitions ($P=1$ for guaranteed moves):\n",
    "\n",
    "From $s_1$:\n",
    "- \"right\" → $s_2$, $P(s_2 \\mid s_1, \\text{right})=1$\n",
    "- \"down\" → $s_3$, $P(s_3 \\mid s_1, \\text{down})=1$\n",
    "\n",
    "From $s_2$:\n",
    "- \"down\" → $s_4$, $P(s_4 \\mid s_2, \\text{down})=1$\n",
    "- \"left\" → $s_1$, $P(s_1 \\mid s_2, \\text{left})=1$\n",
    "\n",
    "From $s_3$:\n",
    "- \"right\" → $s_4$, $P(s_4 \\mid s_3, \\text{right})=1$\n",
    "- \"up\" → $s_1$, $P(s_1 \\mid s_3, \\text{up})=1$\n",
    "\n",
    "Reward Function $R(s, a, s')$:\n",
    "- $R(s, a, s_4)=+1$ (reaching the goal state $s_4$).\n",
    "- $R(s, a, s')=0$ for all other transitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Value Function $v_{\\pi}(s)$**\n",
    "\n",
    "The state-value function $v_{\\pi}(s)$ under policy $\\pi$ is the expected return when starting in state $s$ and following policy $\\pi$. It is defined as:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]\n",
    "$$\n",
    "\n",
    "For a deterministic policy, this simplifies to:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma v_{\\pi}(s')\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Action-Value Function** \n",
    "$q_{\\pi}(s, a)$\n",
    "\n",
    "The action-value function $q_{\\pi}(s, a)$ gives the expected return for taking action $a$ in state $s$ and following $\\pi$ afterward:\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma v_{\\pi}(s')]\n",
    "$$\n",
    "\n",
    "It relates to $v_{\\pi}(s)$ as:\n",
    "\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a \\mid s) q_{\\pi}(s, a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Example Walkthrough in 2x2 Maze**\n",
    "Initial Setup\n",
    "Let’s compute $q_{\\pi}(s,a)$ and $v_{\\pi}(s)$ for a simple deterministic policy $\\pi$, where the agent:\n",
    "\n",
    "- Moves \"right\" from $s_1$,\n",
    "- Moves \"down\" from $s_2$,\n",
    "- Moves \"right\" from $s_3$,\n",
    "- Does nothing in $s_4$ (terminal).\n",
    "\n",
    "Step 1: Compute $v_{\\pi}(s)$ for $s_4$\n",
    "\n",
    "Since $s_4$ is terminal:\n",
    "\n",
    "$$v_{\\pi}(s_4) = 0$$\n",
    "\n",
    "Step 2: Compute $q_{\\pi}(s,a)$ for $s_3$\n",
    "\n",
    "From $s_3$, the only action is \"right\" to $s_4$, $P(s_4 \\mid s_3, \\text{right}) = 1$:\n",
    "\n",
    "$$q_{\\pi}(s_3, \\text{right}) = R(s_3, \\text{right}, s_4) + \\gamma v_{\\pi}(s_4)$$\n",
    "$$q_{\\pi}(s_3, \\text{right}) = 1 + \\gamma (0) = 1$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$v_{\\pi}(s_3) = q_{\\pi}(s_3, \\text{right}) = 1$$\n",
    "\n",
    "Step 3: Compute $q_{\\pi}(s,a)$ for $s_2$\n",
    "\n",
    "From $s_2$, the only action is \"down\" to $s_4$, $P(s_4 \\mid s_2, \\text{down}) = 1$:\n",
    "\n",
    "$$q_{\\pi}(s_2, \\text{down}) = R(s_2, \\text{down}, s_4) + \\gamma v_{\\pi}(s_4)$$\n",
    "$$q_{\\pi}(s_2, \\text{down}) = 1 + \\gamma (0) = 1$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$v_{\\pi}(s_2) = q_{\\pi}(s_2, \\text{down}) = 1$$\n",
    "\n",
    "Step 4: Compute $q_{\\pi}(s,a)$ for $s_1$\n",
    "\n",
    "From $s_1$, the agent moves \"right\" to $s_2$, $P(s_2 \\mid s_1, \\text{right}) = 1$:\n",
    "\n",
    "$$q_{\\pi}(s_1, \\text{right}) = R(s_1, \\text{right}, s_2) + \\gamma v_{\\pi}(s_2)$$\n",
    "$$q_{\\pi}(s_1, \\text{right}) = 0 + \\gamma (1) = \\gamma$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$v_{\\pi}(s_1) = q_{\\pi}(s_1, \\text{right}) = \\gamma$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Policy Probability Distribution\n",
    "For this deterministic policy $\\pi$, the probability distribution over actions is:\n",
    "\n",
    "$$\n",
    "\\pi(\\text{right} \\mid s_1) = 1, \\quad \\pi(\\text{down} \\mid s_1) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi(\\text{down} \\mid s_2) = 1, \\quad \\pi(\\text{left} \\mid s_2) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi(\\text{right} \\mid s_3) = 1, \\quad \\pi(\\text{up} \\mid s_3) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi(\\text{noop} \\mid s_4) = 1 \\quad (\\text{terminal})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary Table**\n",
    "\n",
    "| State | Action | $P(s' \\mid s, a)$ | $R(s, a, s')$ | $q_{\\pi}(s, a)$ | $v_{\\pi}(s)$ |State | Action | $P(s' \\mid s, a)$ | $R(s, a, s')$ | $q_{\\pi}(s, a)$ | $v_{\\pi}(s)$ |\n",
    "|-------|--------|-------------------|---------------|-----------------|--------------|-------|--------|-------------------|---------------|-----------------|--------------|\n",
    "| $s_1$ | right  | $P(s_2 \\mid s_1, \\text{right}) = 1$ | 0 | $\\gamma$ | $\\gamma$ |$s_1$ | right  | $P(s_2 \\mid s_1, \\text{right}) = 1$ | 0 | $\\gamma$ | $\\gamma$ |\n",
    "| $s_2$ | down   | $P(s_4 \\mid s_2, \\text{down}) = 1$  | 1 | 1 | 1 | $s_2$ | down   | $P(s_4 \\mid s_2, \\text{down}) = 1$  | 1 | 1 | 1 |\n",
    "| $s_3$ | right  | $P(s_4 \\mid s_3, \\text{right}) = 1$ | 1 | 1 | 1 | $s_3$ | right  | $P(s_4 \\mid s_3, \\text{right}) = 1$ | 1 | 1 | 1 |\n",
    "| $s_4$ | -      | -                                 | - | 0 | 0 |$s_4$ | -      | -                                 | - | 0 | 0 |\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The MDP, through its rewards and transition probabilities, allows us to calculate action-value functions ($q_{\\pi}(s, a)$) and state-value functions ($v_{\\pi}(s)$) to evaluate policies. These guide the agent's actions to maximize long-term returns, with the optimal policy emerging as the one that maximizes $q_{\\pi}(s, a)$ for all states and actions.MDP, through its rewards and transition probabilities, allows us to calculate action-value functions ($q_{\\pi}(s, a)$) and state-value functions ($v_{\\pi}(s)$) to evaluate policies. These guide the agent's actions to maximize long-term returns, with the optimal policy emerging as the one that maximizes $q_{\\pi}(s, a)$ for all states and actions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
