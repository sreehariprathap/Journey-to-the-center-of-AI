{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming in Reinforcement Learning\n",
    "\n",
    "This notebook provides an introduction to Dynamic Programming (DP) and its relevance to Reinforcement Learning (RL).\n",
    "\n",
    "## Topics Covered:\n",
    "- What is DP?\n",
    "- Policy Evaluation\n",
    "- Policy Improvement\n",
    "- Value Iteration\n",
    "- Implementation with simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Dynamic Programming\n",
    "Dynamic Programming (DP) is a method used in Reinforcement Learning to solve problems where an agent interacts with an environment to maximize rewards.\n",
    "\n",
    "### Bellman Equation\n",
    "The Bellman Equation expresses the value of a state as the expected return starting from that state and following a given policy.\n",
    "\n",
    "$ V(s) = \\mathbb{E} [ R + \\gamma V(s') ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Evaluation\n",
    "Policy evaluation calculates the value function for a given policy. It iteratively updates state values using the Bellman expectation equation:\n",
    "\n",
    "$ V(s) = \\sum_a \\pi(a|s) \\sum_{s',r} P(s', r | s, a) [ r + \\gamma V(s') ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation Example (Gridworld)\n",
    "import numpy as np\n",
    "\n",
    "gamma = 1.0  # Discount factor\n",
    "V = np.zeros(4)  # Assume a simple 4-state environment\n",
    "rewards = np.array([0, -1, -1, 10])  # Rewards for each state\n",
    "\n",
    "for _ in range(10):  # Iterate policy evaluation\n",
    "    for s in range(4):\n",
    "        V[s] = rewards[s] + gamma * (V[s-1] if s > 0 else 0)\n",
    "\n",
    "print(\"State Values:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Improvement\n",
    "Once we evaluate a policy, we can improve it by selecting actions that maximize future rewards. The policy improvement step updates the policy based on the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement Example\n",
    "policy = [0, 1, 1, 0]  # Example policy\n",
    "\n",
    "def improve_policy(V):\n",
    "    return [np.argmax([V[s-1] if s > 0 else 0, V[s]]) for s in range(4)]\n",
    "\n",
    "new_policy = improve_policy(V)\n",
    "print(\"Improved Policy:\", new_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Value Iteration\n",
    "Instead of evaluating policies iteratively, we can directly compute the optimal value function using:\n",
    "\n",
    "$ V(s) \\leftarrow \\max_a \\sum_{s',r} P(s', r | s, a) [ r + \\gamma V(s') ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Example\n",
    "V = np.zeros(4)\n",
    "\n",
    "for _ in range(10):  # Iterate value iteration\n",
    "    for s in range(4):\n",
    "        V[s] = max(rewards[s] + gamma * (V[s-1] if s > 0 else 0), V[s])\n",
    "\n",
    "print(\"Optimal State Values:\", V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
