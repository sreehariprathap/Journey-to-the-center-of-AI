{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction\n",
    "\n",
    "### Definition 1\n",
    "Reinforcement Learning is learning what to do-how to map situations to actions-so as to\n",
    "maximize a numerical reward signal.\n",
    "### Definition 2\n",
    "Reinforcement Learning (RL) is a type of machine learning paradigm in which an agent learns to\n",
    "make a sequence of decisions by interacting with an environment. The agent receives feedback in\n",
    "the form of rewards or penalties, enabling it to learn optimal strategies over time.\n",
    "- Learning by trial-and-error with a delayed reward\n",
    "- Actions are taken sequentially, current decisions affect future outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components of RL\n",
    "\n",
    "![Reinforcement Learning Model](https://images.spiceworks.com/wp-content/uploads/2022/09/29100907/Reinforcement-Learning-Model.png)\n",
    "\n",
    "### Environment\n",
    "The external system with which the agent interacts,\n",
    "providing feedback in the form of rewards\n",
    "### Agent\n",
    "The learner and decision-maker that interacts with the\n",
    "environment\n",
    "### State (s)\n",
    "Full representation of the environment\n",
    "### Action (a)\n",
    "Decision taken by agent at a given state to transition it\n",
    "to another state\n",
    "### Reward (r)\n",
    "A numerical representation that quantifies the\n",
    "immediate feedback after the agent takes an action at a\n",
    "state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Learning Paradigms\n",
    "\n",
    "| Learning Paradigm      | How They Learn                                           | Interaction with Environment | Feedback Mechanism          |\n",
    "|------------------------|----------------------------------------------------------|------------------------------|-----------------------------|\n",
    "| Supervised Learning    | Learns from labeled data with input-output pairs         | Passive: Receives predefined labels | Direct, Explicit            |\n",
    "| Unsupervised Learning  | Learns patterns and structures in unlabeled data         | Passive: Learns patterns from data  | Indirect, Intrinsic Structures |\n",
    "| Reinforcement Learning | Learns from interaction with the environment, receiving delayed feedback in the form of rewards | Active: Takes actions in the environment | Delayed, Scalar Rewards      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Advantages of RL\n",
    "\n",
    "- Focuses on the long-term goal\n",
    "- Easy data collection process\n",
    "- Learn from interaction\n",
    "- Operates in an evolving & uncertain environment\n",
    "- Sequential Decision-Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL Problem\n",
    "\n",
    "The Reinforcement Learning (RL) problem is a framework in machine learning where an agent interacts with an environment over a series of discrete time steps. The agent's goal is to learn a policy that maximizes the cumulative rewards it receives from the environment.\n",
    "\n",
    "RL explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. This contrasts with many approaches that consider subproblems without addressing how they might fit into a larger picture.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **Environment, Agent, State, Action, Reward**\n",
    "- **Policy (Ï€)**: The strategy or mapping from states to actions that the agent follows. The goal of RL is to learn an optimal policy that maximizes the expected cumulative rewards.\n",
    "- **Value Function (V)**: The expected cumulative rewards that an agent can expect to receive from a given state onwards, following a specific policy. It helps the agent evaluate the desirability of different states.\n",
    "- **Model (Optional)**: A learned or known model of the environment's dynamics, which provides information about how the environment will respond to the agent's actions. Models are not always used, but when available, they can assist in planning and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation\n",
    "\n",
    "Exploration and exploitation are key aspects of RL. There is always a trade-off between the two, where the agent must balance between:\n",
    "- Taking new actions to explore the environment and gather information\n",
    "- Taking the best known strategy so far in a greedy fashion to maximize the reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Applications\n",
    "\n",
    "### Gaming\n",
    "Teaching agents to play games like chess, Go, pong, or video games.\n",
    "\n",
    "### Robotics\n",
    "Training robots to perform tasks in real-world environments such as robotic arms for pick-and-place or path planning for ground vehicles.\n",
    "\n",
    "### Finance\n",
    "Optimizing investment strategies in financial markets.\n",
    "\n",
    "### Healthcare\n",
    "Personalizing treatment plans based on patient data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
