{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abbd840",
   "metadata": {},
   "source": [
    "# Chain Rule and Gradient\n",
    "This notebook provides a guide to understanding the **Chain Rule** and **Gradients**, their applications, and their role in AI.\n",
    "\n",
    "## Contents:\n",
    "- Chain Rule\n",
    "- Gradient\n",
    "- Applications in AI\n",
    "- Symbolic and Numerical Computations using Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95a079",
   "metadata": {},
   "source": [
    "## 1. Chain Rule\n",
    "### Definition:\n",
    "The Chain Rule allows us to differentiate composite functions by breaking them down into their components. It’s represented as:\n",
    "$$ (f(g(x)))' = f'(g(x)) \\cdot g'(x) $$\n",
    "\n",
    "### Example:\n",
    "Let’s calculate the derivative of a composite function $h(x) = \\sin(x^2)$ using the Chain Rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Applying the Chain Rule\n",
    "from sympy import symbols, sin, diff\n",
    "x = symbols('x')\n",
    "h = sin(x**2)\n",
    "dh_dx = diff(h, x)\n",
    "dh_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3d12a",
   "metadata": {},
   "source": [
    "### Application in AI\n",
    "The Chain Rule is critical in **backpropagation** for training neural networks, where it’s used to compute gradients of the loss function with respect to each layer's parameters by working from the output layer back to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46aad25",
   "metadata": {},
   "source": [
    "## 2. Gradient\n",
    "### Definition:\n",
    "The **gradient** of a function is a vector of partial derivatives, which indicates the direction and rate of the fastest increase of the function. It’s denoted as:\n",
    "$$ \\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right) $$\n",
    "\n",
    "### Example:\n",
    "Let's calculate the gradient of a multivariable function $f(x, y) = x^2 + y^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc715f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient calculation of a multivariable function\n",
    "y = symbols('y')\n",
    "f = x**2 + y**2\n",
    "df_dx = diff(f, x)\n",
    "df_dy = diff(f, y)\n",
    "(df_dx, df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e0480",
   "metadata": {},
   "source": [
    "### Application in AI: Gradient Descent\n",
    "In AI, the **gradient** helps optimize loss functions. The gradient indicates the direction in which parameters should be adjusted to minimize the error.\n",
    "\n",
    "### Example of Gradient Descent:\n",
    "For a function $J(\\theta) = \\theta^2$, we can find the gradient and update $\\theta$ iteratively to reach the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9835db",
   "metadata": {},
   "source": [
    "### Symbolic Computation Using Python (SymPy)\n",
    "Using SymPy, we can compute symbolic derivatives and gradients. Below, we’ll apply the Chain Rule and calculate gradients for a multivariable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Rule on another composite function\n",
    "g = x**3 + 2*x\n",
    "f_g = sin(g)\n",
    "df_g_dx = diff(f_g, x)\n",
    "df_g_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of a multivariable function\n",
    "f_multi = x**2 + 3*x*y + y**2\n",
    "df_dx_multi = diff(f_multi, x)\n",
    "df_dy_multi = diff(f_multi, y)\n",
    "(df_dx_multi, df_dy_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2af711",
   "metadata": {},
   "source": [
    "### Numerical Computation Using Python (SciPy)\n",
    "For complex functions, we often use numerical approaches for calculating gradients and applying gradient descent.\n",
    "\n",
    "In this example, we compute the gradient of a simple function numerically and observe gradient descent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acff6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical gradient approximation using SciPy and Numpy\n",
    "from scipy.optimize import approx_fprime\n",
    "import numpy as np\n",
    "\n",
    "def func(v):\n",
    "    x, y = v\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Point to calculate the gradient at\n",
    "point = np.array([1.0, 1.0])\n",
    "\n",
    "# Approximate gradient at the point\n",
    "epsilon = np.sqrt(np.finfo(float).eps)\n",
    "gradient = approx_fprime(point, func, epsilon)\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321414c",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "The Chain Rule and Gradient are fundamental in AI, especially for optimizing models. In this notebook, we explored their definitions, applications, symbolic, and numerical computations. Understanding these tools is key for effective model training in AI."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
